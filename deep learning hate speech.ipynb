{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "\n",
    "# Time\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Tools\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from pywsd.utils import lemmatize_sentence\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight as cw\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import f1_score, accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "# Deep Learing Preprocessing - Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Deep Learning Model - Keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Deep Learning Model - Keras - CNN\n",
    "from keras.layers import Conv1D, Conv2D, Convolution1D, MaxPooling1D, SeparableConv1D, SpatialDropout1D, \\\n",
    "    GlobalAvgPool1D, GlobalMaxPool1D, GlobalMaxPooling1D \n",
    "from keras.layers.pooling import _GlobalPooling1D\n",
    "from keras.layers import MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "# Deep Learning Model - Keras - RNN\n",
    "from keras.layers import Embedding, LSTM, Bidirectional\n",
    "\n",
    "# Deep Learning Model - Keras - General\n",
    "from keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU, PReLU, Lambda, Multiply\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning Parameters - Keras\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# Deep Learning Callbacs - Keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    " \n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_excel(\"hatespeechdata.xlsx\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial data cleaning requirements that we can think of after looking at the top 5 records:\n",
    "The Twitter handles are already masked as @user due to privacy concerns. So, these Twitter handles are hardly giving any information about the nature of the tweet. We can also think of getting rid of the punctuations, numbers and even special characters since they wouldn’t help in differentiating different kinds of tweets. Most of the smaller words do not add much value. For example, ‘pdx’, ‘his’, ‘all’. So, we will try to remove them as well from our data. Once we have executed the above three steps, we can split every tweet into individual words or tokens which is an essential step in any NLP task.We also need to reduce words to their roots eg loved , loves , loving . These terms are often used in the same context. If we can reduce them to their root word, which is ‘love’, then we can reduce the total number of unique words in our data without losing a significant amount of information.\n",
    "\n",
    "Given below is a user-defined function to remove unwanted text patterns from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "data['tidy_tweet'] = np.vectorize(remove_pattern)(data['v2'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "data['tidy_tweet'] = np.vectorize(remove_pattern)(data['v2'], \"#[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "data['tidy_tweet'] = data['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing short words\n",
    "data['tidy_tweet'] = data['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHead():\n",
    "    data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tokenized_tweet = data['tidy_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()\n",
    "all_words = ' '.join([text for text in data['tidy_tweet']])\n",
    "#test = join(data,'tidy_tweet')\n",
    "#cloud(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "tokenized_tweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "data['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = data[\"v1\"]\n",
    "texts = data[\"tidy_tweet\"]\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max = 1000\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags)\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)\n",
    "mat_texts = tok.texts_to_matri(texts,mode='count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(texts,tags, test_size = 0.2)\n",
    "mat_texts_tr = tok.texts_to_matrix(x_train,mode='count')\n",
    "mat_texts_tst = tok.texts_to_matrix(x_test,mode='count')\n",
    "\n",
    "max_len = 100\n",
    "x_train = tok.texts_to_sequences(x_train)\n",
    "x_test = tok.texts_to_sequences(x_test)\n",
    "cnn_texts_mat = sequence.pad_sequence(x_train,maxlen=max_len)\n",
    "max_len = 100\n",
    "cnn_texts_mat_tst = sequence.pad_sequences(x_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model_v2():   \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(1000,50,input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m_CNN_v2 = get_cnn_model_v2()\n",
    "#check_model(m,cnn_texts_mat,y_train,cnn_texts_mat_tst ,y_test)\n",
    "history_cnn_v2 = m_CNN_v2.fit(cnn_texts_mat,y_train, epochs=10, batch_size=60, validation_split=0.2)\n",
    "acc = history_cnn_v2.history['acc']\n",
    "val_acc = history_cnn_v2.history['val_acc']\n",
    "loss = history_cnn_v2.history['loss']\n",
    "val_loss = history_cnn_v2.history['val_loss']\n",
    "epochs = range(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, '-', color='red', label='training acc')\n",
    "plt.plot(epochs, val_acc, '-', color='green', label='validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, '-', color='red', label='training acc')\n",
    "plt.plot(epochs, val_loss,  '-', color='green', label='validation acc')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m_CNN_v2.predict_classes(cnn_texts_mat_tst)\n",
    "acc = m_CNN_v2.evaluate(cnn_texts_mat_tst ,y_test)\n",
    "proba_cnn_v2 = m_CNN_v2.predict_proba(cnn_texts_mat_tst)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Test loss is {0:.4f} accuracy is {1:.4f}  \".format(acc[0],acc[1]))\n",
    "print(confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(cm,title):\n",
    "    df_cm2 = pd.DataFrame(cm, index = ['normal', 'hate'])\n",
    "    df_cm2.columns=['normal','hate']\n",
    "\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(df_cm2, annot=True, fmt=\"d\", linewidths=.5,ax=ax)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(confusion_matrix(pred, y_test),'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LSTM_model():    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(1000, 32))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "m_LSTM = get_LSTM_model()\n",
    "#check_model(m,cnn_texts_mat,y_train,cnn_texts_mat_tst ,y_test)\n",
    "history_lstm = m_LSTM.fit(cnn_texts_mat,y_train, epochs=10, batch_size=60, validation_split=0.20)\n",
    "acc = history_lstm.history['acc']\n",
    "val_acc = history_lstm.history['val_acc']\n",
    "loss = history_lstm.history['loss']\n",
    "val_loss = history_lstm.history['val_loss']\n",
    "epochs = range(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, '-', color='red', label='training acc')\n",
    "plt.plot(epochs, val_acc, '-', color='green', label='validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, '-', color='red', label='training acc')\n",
    "plt.plot(epochs, val_loss,  '-', color='green', label='validation acc')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m_LSTM.predict_classes(cnn_texts_mat_tst)\n",
    "acc = m_LSTM.evaluate(cnn_texts_mat_tst ,y_test)\n",
    "proba_lstm = m_LSTM.predict_proba(cnn_texts_mat_tst)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Test loss is {0:.2f} accuracy is {1:.4f}  \".format(acc[0],acc[1]))\n",
    "print(confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(confusion_matrix(pred, y_test),'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_lstm = m_LSTM.predict(cnn_texts_mat_tst).ravel()\n",
    "fpr_lstm, tpr_lstm, thresholds_lstm = roc_curve(y_test, y_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "from sklearn.metrics import auc\n",
    "auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
    "auc_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_CNN = m_CNN_v2.predict(cnn_texts_mat_tst).ravel()\n",
    "fpr_CNN, tpr_CNN, thresholds_CNN = roc_curve(y_test, y_pred_CNN)\n",
    "from sklearn.metrics import auc\n",
    "auc_CNN = auc(fpr_CNN, tpr_CNN)\n",
    "auc_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lstm, tpr_lstm, label='LSTM (area = {:.3f})'.format(auc_lstm))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_CNN, tr_CNN,label='CNN (area = {:.3f})'.format(auc_CNN))\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
